\documentclass{article}

\usepackage{amsmath, amssymb}
\newtheorem{definition}{Definition}

\begin{document}

\title{Mit 6.043 - Artificial Intelligence}
\author{Marco Filippone}
\maketitle

\section{Reasoning: goal trees and rule-based expert systems}

A \textbf{representation} consists of the following four
fundamental parts:
\begin{itemize}
  \item A lexical part that determines which symbols are allowed
    in the representation's vocabulary
  \item A structural part that describes constraints on how the
    symbols can be arranged
  \item A procedural part that specifies access procedures that
    enable you to create descriptions, to modify them, and to
    answer questions using them
  \item A semantic part that establishes a way associating a
    meaning with the description
\end{itemize}

A \textbf{semantic net} is a representation in which:
\begin{itemize}
  \item Lexically, there are nodes, links, and application-specific link labels
  \item Structurally, each link conects a tail node to a head node
  \item Semantically, the nodes and links denote application-specific entities
\end{itemize}
With constructors that:
\begin{itemize}
  \item Construct a node
  \item Construct a link, given a link label and two nodes to be connected
\end{itemize}
With readers that:
\begin{itemize}
  \item Produce a list of all links departing from a given node
  \item Produce a list of all links arriving at a given node
  \item Produce a tail node, given a link
  \item Produce a head node, given a link
  \item Produce a link label, given a link
\end{itemize}

A \textbf{Semantic tree} is a representation, that is a semantic net in which:
\begin{itemize}
  \item certain links are called \textbf{branches}. Each banch
    connects two nodes; the head node is called the \textbf{parent
    node} and the tail node is called the \textit{child node}
  \item One node has no parent; it is called the root node.
    Other nodes have exactly one parent
  \item Some nodes have no children, they are called
    \textit{leaf nodes}. When two nodes are connected to each
    other by a chain of two or more branches, one is said to be
    the \textit{ancestor}; the other if said to be the
    descendant
\end{itemize}

With constructors that:
\begin{itemize}
\item Connect a parent node to a child node with a branch links
\end{itemize}
With readers that:
\begin{itemize}
\item Produce a list of a given node's children
\item Produce a given node's parent
\end{itemize}

A \textbf{goal tree} is a semantic tree in which: nodes represent goals and branches indicate how you can achive goals by solving one or more subgoals. Each node's children corresponds to \textbf{immediate subgoals}; each node's parent corresponds to the \textbf{immediate supergoal}. The top node, the one with no parent, is the \textbf{root} goal.

Some goals are satisfied directly, without reference to any other subgoals. These goals are called \textbf{leaf goals}, and the corresponding nodes are called \textbf{leaf nodes}.

Because goal trees always involve \textit{And} nodes, or \textit{Or} nodes, or both. they are often called \textbf{And-Or trees}.

To determine whether a goal has been achieved, you need a testing procedure. The key procedure, \textit{REDUCE}, channels action into the \textit{REDUCE-AND} and the \textit{REDUCE-OR}.

Goal trees enable introspective question answering:
\begin{itemize}
  \item how: the immediate subgoal (downstream) 
  \item why the immediate supergoal (downstream) 
\end{itemize}

\subsection{Eliciting expert systems features}
\begin{itemize}
  \item Heuristic of specific situations: it is dangerous to limit inquiry to office interviews
  \item Heuristic of situation comparison: ask a domain expert for clarification whenever the domain expert's behavior varies in situations that look identical to the knowledge enginner.
  \item You should build a system and see when it cracks. Helps identifiying missing rules.
\end{itemize}

\section{Nets and Basic Search}

A \textbf{search tree} is a representation, that is a semantic tree, in which:
\begin{itemize}
  \item Nodes denote paths
  \item Branches connect paths to one-set path extensions
\end{itemize}
With writers that:
\begin{itemize}
  \item Connect a path to a path description
\end{itemize}
WIth reades that:
\begin{itemize}
  \item Produce a path's description
\end{itemize}

Each child denotes a path that is a one-city extension of the path denoted by its parent.

If a node has \textit{b} children, it is said to have a \textbf{branching}
factor of \textit{b}. If the number of children is always \textit{b}
for every nonleaf node, the the tree is said to have a branching 
factor of \textit{b}.

Each path that does not reach the goal is called a \textbf{partial path}.
Each path that does reach the goal is called a \textbf{complete path},
and the corresponding node is called a \textbf{goal node}.

Nodes are said to be \textbf{open} until they can be expanded,
whereupon they become \textbf{closed}.

\subsection{Blind methods}

\subsubsection{Depth-first Search}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path
    that contains only the root node
  \item Until the first path in the queue terminates at the goal node or the queue is empty,
    \begin{itemize}
      \item Remove the first path from the queue; create new paths
        by extending the first path to all the neighbors of the 
        terminal node
      \item Reject all new paths with loops
      \item Add the new paths, if any to the \textbf{front} of the queue
    \end{itemize}
  \item If the goal node is found, annouce success; otherwise, annouce 
    failure
\end{itemize}

Good when you are confident that all partial paths either reach dead ends
or become complete paths after a reasonable number of steps.

Bad with long or infinite paths, that neither reach dead ends 
nor become complete paths.

\subsubsection{Breadth-first Search}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path
    that contains only the root node
  \item Until the first path in the queue terminates at the goal node 
    or the queue is empty,
    \begin{itemize}
      \item Remove the first path from the queue; create new paths
        by extending the first path to all the neighbors of the 
        terminal node
      \item Reject all new paths with loops
      \item Add the new paths, if any to the \textbf{back} of the queue
    \end{itemize}
  \item If the goal node is found, annouce success; otherwise, annouce 
    failure
\end{itemize}


Works with trees with infinitely deep paths.

Wasteful when all paths lead to the goal node at more or less the same depth.

\subsubsection{Nondeterministic search}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path
    that contains only the root node
  \item Until the first path in the queue terminates at the goal node 
    or the queue is empty,
    \begin{itemize}
      \item Remove the first path from the queue; create new paths
        by extending the first path to all the neighbors of the 
        terminal node
      \item Reject all new paths with loops
      \item Add the new paths \textbf{at random places in the queue}
    \end{itemize}
  \item If the goal node is found, annouce success; otherwise, annouce 
    failure
\end{itemize}

\subsection{Heuristically informed methods}

\subsubsection{Hill climbing}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path
    that contains only the root node
  \item Until the first path in the queue terminates at the goal node 
    or the queue is empty,
    \begin{itemize}
      \item Remove the first path from the queue; create new paths
        by extending the first path to all the neighbors of the 
        terminal node
      \item Reject all new paths with loops
      \item \textbf{Sort the new paths, if any, by the estimated distances 
        between their terminal nodes and the goal}
      \item Add the new paths, if any to the \textbf{front} of the queue
    \end{itemize}
  \item If the goal node is found, annouce success; otherwise, annouce 
    failure
\end{itemize}

Possible issues:
\begin{itemize}
  \item Foothills: an optimal point is found, but it is a local maximum
  \item Plateaus: for all but a small number of positions, all standard-step
    probes leave the quality measurement unchanged
  \item Ridges
\end{itemize}

\subsubsection{Beam search}

Like BFS in that ir progresses level by level. Unlike BFS, beam 
search moves downward only through the best \textit{w} 
nodes at each level; the other nodes are ignored.

\subsubsection{Beast-first search}

While hill climbing demands forward motion from the most recently created
open node. In the best-first seach, \textbf{forward motion is 
from the best open node so far} no matter where that node is i the
partially developed tree.

\subsection{Which search type is good for me?}
\begin{itemize}
  \item Depth-first search is good when unproductive partial
    paths are never too long
  \item Breadth-first search is good when the branching 
    factor is never too large
  \item Nondeterministic search is good when you are not sure whether
    depth-first search or breadth-first search would be better
  \item Hill climbing is good when there is a natural measurement
    of goal distance form each place to the goal and a good path is 
    likely to be among the partial paths that appear to be good at
    all levels
  \item Beam search is good when there is a natural measure of
    goal distance and a good path is likely to be among the 
    partial paths that appear to be good at all levels
  \item Best-first search is good when there is a natural measure 
    of the goal distance and a good partial path may look like
    a bad option before more promising partial paths are played
    out
\end{itemize}

\section{Nets and Optimal Search}

\subsection{British Museum procedure}

Find all possible paths and select the best one from them.

\subsection{Branch and Bound}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path 
    that contains only the root node
  \item Until the first path in the queue terminates at the 
    goal node or the queue is empth:
    \begin{itemize}
      \item Remove the first path from the queue; create new paths 
        by extending the first path to all the neighbors
        of the terminal node
      \item Reject all new paths with loops
      \item add the remaining new paths, if any, to the queue
      \item Sort the entire queue by path length with 
        least-cost paths in front
    \end{itemize}
  \item If the goal node is gound, annouce success; otherwise, 
    annouce failure
\end{itemize}

\subsection{Branch and Bound with lower-bound estimate}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path 
    that contains only the root node
  \item Until the first path in the queue terminates at the 
    goal node or the queue is empth:
    \begin{itemize}
      \item Remove the first path from the queue; create new paths 
        by extending the first path to all the neighbors
        of the terminal node
      \item Reject all new paths with loops
      \item Add the remaining new paths, if any, to the queue
      \item Sort the entire queue by \textbf{the sum of the path
        length and a lower-bound estimate of the cost
        remaining, with least-cost paths in front}
    \end{itemize}
  \item If the goal node is gound, annouce success; otherwise, 
    annouce failure
\end{itemize}

\subsection{Branch and Bound with dynamic programming}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path 
    that contains only the root node
  \item Until the first path in the queue terminates at the 
    goal node or the queue is empth:
    \begin{itemize}
      \item Remove the first path from the queue; create new paths 
        by extending the first path to all the neighbors
        of the terminal node
      \item Reject all new paths with loops
      \item Add the remaining new paths, if any, to the queue
      \item \textbf{if two or more paths reach a common node,
        delete all those paths except the one that reaches the
        common node with the minimum cost}
      \item Sort the entire queue by \textbf{the sum of the path
        length with least-cost paths in front}
    \end{itemize}
  \item If the goal node is gound, annouce success; otherwise, 
    annouce failure
\end{itemize}

\subsection{A* procedure - Branch and bound with Underestimates
and Dynamic Programming}

\begin{itemize}
  \item Form a one-element queue consisting of a zero-length path 
    that contains only the root node
  \item Until the first path in the queue terminates at the 
    goal node or the queue is empth:
    \begin{itemize}
      \item Remove the first path from the queue; create new paths 
        by extending the first path to all the neighbors
        of the terminal node
      \item Reject all new paths with loops
      \item Add the remaining new paths, if any, to the queue
      \item \textbf{if two or more paths reach a common node,
        delete all those paths except the one that reaches the
        common node with the minimum cost}
      \item Sort the entire queue by \textbf{the sum of the path
        length and a lower-bound estimate of the cost
        remaining, with least-cost paths in front}
    \end{itemize}
  \item If the goal node is gound, annouce success; otherwise, 
    annouce failure
\end{itemize}

\subsection{Which optimal search method is good for me?}
\begin{itemize}
  \item The British Museum procedure is good only when the 
    search tree is small
  \item Branch-and-bound search is good when the tree is big 
    and bad paths turn distinctly bad quickly
  \item Branch-and-bound search with a guess is good when 
    there is a good lower-bound estimate of the distance
    remaining to the goal
  \item Dynamic programming is good when many paths convers
    on the same place
  \item The A* procedure is good when both branch-and-bound 
    search with a guess and dynamic programming are good
\end{itemize}

\section{Trees and Adversarial Search}

A \textbf{game tree} is a representation:
\begin{itemize}
  \item Nodes denote board configuration
  \item Branches denote moves
\end{itemize}
With writers that:
\begin{itemize}
  \item Establish that a node is for the maximizer or for
    the minimizer
  \item Connect a board configuration with a board-configuration
    description
\end{itemize}
With readers that:
\begin{itemize}
  \item Determine whether the node if for the minimizer
    or for the maximizer
  \item Produce a board configuration's description
\end{itemize}

\subsection{Min-Max procedure}

\begin{itemize}
  \item If the limit of the search has been reached, compute 
    the static value of the current position relative to the
    appropriate player. Report the result.
  \item Otherwise, if the level is a minimizing level, use
    MINMAX on the children of the current position. Report 
    the minimum of the results
  \item Otherwise, the level is a minimizing level. Use
    MINMAX on the children of the current position. Report 
    the maximum of the results
\end{itemize}

\subsection{Alpha-Beta procedure}

\begin{itemize}
  \item If the level is the top level, let alpha be
    \(-\infty\) and let beta be \(+\infty\)
  \item If the limit of search has been reached, compute the
    static value of the current position relative to the
    appropriate player. Report the result
  \item If the level is a minimizing level:
    \begin{itemize}
      \item Until all children are examined with ALPHA-BETA or
        until alpha is equal to or greater than beta:
        \begin{itemize}
          \item Use ALPHA-BETA procedure, with the current alpha
            and beta values, on a child; note the value reported
          \item Compare the value reported with the beta value;
            if the reported value is smaller, reset beta to the
            new value
          \item Report beta
        \end{itemize}
    \end{itemize}
      \item Otherwise, the level is a maximizing level:
    \begin{itemize}
      \item Until all children are examined with ALPHA-BETA or
        until alpha is equal to or greater than beta:
        \begin{itemize}
          \item Use ALPHA-BETA procedure, with the current alpha
            and beta values, on a child; note the value reported
          \item Compare the value reported with the beta value;
            if the reported value is larger, reset alpha to the
            new value
          \item Report alpha
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Heuristic Methods}

\subsubsection{Progressive deepening}

For each depth level, compute the best move. By doing so, to compute up to level $d-1$ it is necessary to
evaluate:

\begin{math}
  b^0 + b^1 + \cdots + b^{d-1} = \frac{b^d-1}{b-1} \approx b-1
\end{math}

\subsubsection{Forced move heuristic}

The child move involving a \textit{forced move} generally has a
value that stands out from the rest.

\subsubsection{Singular-extension heuristic}

The search should continue as long as one move's static value
stands out.

\subsubsection{Tapered search}

Arrange for the branching factor to vary with depth of
penetration, possibly using tapered search to direct more effort
into the more promising moves.

\section{Symbolic constraints and propagation}

\paragraph{Principle of convergent intelligence} The world
manifest constraints and regularities. If a computer is to
exhibit intelligence, it must exploit those constraints and
regularities, no matter of what the computer happens to be made.

\paragraph{Describe-to-explain principle} The act of detailed
description may turn probabilistic regularities into entirely
deterministic constrains.

\paragraph{Marr's methodological principles}
\begin{enumerate}
  \item Identify the problem
  \item Select or develop an appropriate representation
  \item Expose constraints or regularities
  \item Create particular procedures 
  \item Verify via experiments
\end{enumerate}

\paragraph{Contraction net} is a representation that is a \textit{frame
system}, in which:
\begin{itemize}
  \item Lexically and structurally, certain frame classes
    identify a finite list of application-specific
    interpretations
  \item Procedurally, demon procerures enforce compatibility
    constraints among connected frames
\end{itemize}

\subsection{Applications}

\subsubsection{3D object recognizion in 2D images}

\paragraph{Labeled drawing} is a representation that is a \textit{frame
system}, in which:
\begin{itemize}
  \item Lexically, there are line frames and junctions frames.
    Lines may be convex, cocave, or boundary lines. Junctions may
    be \textit{L}, \textit{Fork}, \textit{T}, or \textit{Arrow}
    junctions
  \item Structurally, junctions frames are connected by line
    frames. Also, each junction frame contains a list of
    interpretation combinations for its connecting lines
  \item Semantically, line frames denotee physical edges.
    Junction frames denote physical vertexes
  \item Procedurally, demon procedures enforce the constraint
    that each junction lavel must be compatible with at least one
    of the junction labels at each of the neighboring junctions
\end{itemize}

\subsubsection{Time-interval relations and scheduling}

\paragraph{Interval net} is a representation that is a
\textit{contraction net}, in which:
\begin{itemize}
  \item Lexically and semantically there are interval frames
    denoting time intervals and lik frames denoting time
    relations: specifically: $\overrightarrow{before}$,
    $\overrightarrow{during}$, $\overrightarrow{overlaps}$,
    $\overrightarrow{meets}$, $\overrightarrow{starts}$,
    $\overrightarrow{finishes}$, $\overrightarrow{is equal to}$,
    and their mirrors
  \item Structurally, interval frames are connected by link
    frames
  \item Procedurally, demon procedures enforce the constraint
    that the interpretations allowed for a link frame between two
    intervals must be consistent with the interpretations allowed
    for the two link frames joining the two intervals to a third
    interval
\end{itemize}

\subsection{Map coloring - Lecture notes}

\begin{definition}
  A \textbf{variable} $v$ is something that can have an assigment
\end{definition}

\begin{definition}
  A \textbf{value} $x$ is something that can be an assigment
\end{definition}

\begin{definition}
  A \textbf{domain} $D$ is a bag of values
\end{definition}

\begin{definition}
  A \textbf{constraint} $c$ is a limit on variable values
\end{definition}

Procedure:
\begin{itemize}
  \item For each \textit{DFS} assignment
  \item For each variable $v_i$ considered.  A \textit{considered 
    variable} is some variable that may affect the assignment decision
  \item For each $x_i$ in $D_i$
  \item For each constraint $c(x_i, x_j)$ where $x_j \in D_j$
  \item If $\nexists x_j$ such that $c(x_i, x_j)$ is satisfied,
    then remove $x_i$ from $D_i$.
  \item If $D_i$ is empty, backtrack
\end{itemize}

Examples of \textit{considered variables}:
\begin{itemize}
  \item Nothing: leads to wrong outputs
  \item Assignment: constraints are too slack
  \item Neighbors' only assignment: can succeed but depends on
    assigment ordering. If most constrained assignments are
    considered first, then it is fast; vice versa, might not end
  \item Propagate checking through variables with reduced domain
    reduced to a single value
  \item Propagate checking through variables with reduced domains 
  \item Everything: too computationally intensive
\end{itemize}

\section{K-D Trees and Nearest Neighbors}

\paragraph{Consistency heuristic} Whenever you want to guess a
property of something, given nothing else to go on but a set of
reference cases, find the most similar case, as measured by known
properties, for which the property is known. Guess that the
unknown property is the same as that known property.

\paragraph{Decision tree} A decision tree is a representation,
that is a semantic tree in which:
\begin{itemize}
  \item Each node is connected to a set of possible answers
  \item Each nonleaf noed is connected to a test that splits its
    set of possible answers into subsets corresponding to
    different test results
  \item Each branch carries a particular test result's subset to
    another node
\end{itemize}

\paragraph{k-d tree} A k-d tree is a representation,
that is a decision tree in which:
\begin{itemize}
  \item The set of possible answers consists of points, one of
    which may be the nearest neighbor to a given point
  \item Each test specifies a coordinate, a threshold, and a
    neutral zone around the threshold containing no points
  \item Each test divides a set of points into two sets, according
    to on which sid of the threshold each point lies
\end{itemize}

How to divide the cases into sets:
\begin{itemize}
  \item If there is only one case, stop
  \item If this is the first division of cases, pick the vertical
    axis for comparison; otherwise, pick the axis that is
    different from the axis at the next higher level
  \item Considering only the axis of comparison, find the average
    position of the two middle objects. Call this average position
    the threshold, and construct a decision-tree test that
    compares unknowns in the axis of comparison againsts the
    threshold. Also note the position of the two middle obkects in
    the axis of comparison. Call these positions the upper and
    lower boundaries
  \item Divide up all the obects into two subsets, accorting to on
    which side of the average position they lies
  \item Divide up thw objects in each subset, forming a subtree
    for each, using this procedure
\end{itemize}

To find the nearest neighbor using the K-D procedure:
\begin{itemize}
  \item Determine whether there is only one element in the set
    under consideration
  \begin{itemize}
    \item If there is only one, report it
    \item Otherwise, compare the unknown, in the axis of
      comparison, against the current node's threshold. The result
      determines the likely set
    \item Find the nearest neighbor in the likely set using this
      procedure
    \item Determine whether the distance to the nearest neighbor
      in the likely set is less than or equal to the distance to
      the other set's boundary in the axis of comparison:
      \begin{itemize}
        \item If it is, then report the nearest neighbor in the
          likely set
        \item If it is not, check the unlikely set using this
          procedure; return the nearer of the nearest neighbors in
          the likely set and in the unlikely set
      \end{itemize}
  \end{itemize}
\end{itemize}

\section{Identication Trees}

\paragraph{Identication Tree} An identification tree is a
represntation, that is a decision tree in which:
\begin{itemize}
  \item Each set of possible conclusion is established implicitly
    by a list of samples of known class
\end{itemize}

\paragraph{Average disorder}
\begin{math}
  Average(disorder) =
  \sum_{b}{\frac{n_b}{n_t} \sum_{c}{-\frac{n_{bc}}{n_b}\log_{2}\frac{n_{bc}}{n_b}}}
\end{math}
where:
\begin{itemize}
  \item $n_b$ is the number of samples in brench $b$
  \item $n_t$ is the total number of samples in all branches
  \item $n_bc$ is the total of samples in branch $b$ of class $c$
\end{itemize}

To generate an identification tree using SPROUTER:
\begin{itemize}
  \item Until each leaf node is populated by as homogeneous a
    sample set as possible:
    \begin{itemize}
      \item Select a leaf node with an inhomogeneous sample set
      \item Replace that leaf node by a test node that divides the
        inhomogeneous sample set into minimally inhomogeneous
        subsets, according to some measure of disorder
    \end{itemize}
\end{itemize}

To convers an identification tree into a rule set, execute the
following procedure - PRUNER:
\begin{itemize}
  \item Create one rule for each root-to-leaf path in the
    identification tree
  \item Simplify each rule by discarding antecedents that have no
    effect on the conclusion reached by the rule
  \item Replace those rules that share the most common consequent
    by a default rule that is triggered when no other rule is
    triggered. In the eventi of a tie, use some heuristic tie
    breaker to choose a default rule
\end{itemize}

\section{Neural Nets}

\paragraph{Neural Net} A neural net is a representation, that is a
arithmetic constraint net in which:
\begin{itemize}
  \item Operation frames denote arithmetic constraints modeling
    synapses and neurons
  \item Demon procedures propagate stimuli through synapses and
    neurons
\end{itemize}
With demon procedures defines assignment:
\begin{itemize}
  \item When a value is written into a synapse's input slot, write
    the product of the value and the synapse's weight into the
    synapse's output slot
  \item When a value is written into a synapse's output slot,
    check the following neuron to see whether all its input
    synapses' outputs have values:
    \begin{itemize}
      \item If they do, add the outputs values of the input
        synapses together, pass the sum through the activation
        function, and write the appropriate value into the
        neuron's output slot
      \item If they don't, do nothing
    \end{itemize}
\end{itemize}

To do back propagation to train a neural net:
\begin{itemize}
  \item Pick a rate parameter $r$
  \item Until performance is satisfactory,
    \begin{itemize}
      \item For each input,
        \begin{itemize}
          \item Compute the resulting output
          \item Compute $\beta$ for nodes in the output layers
            using $\beta_{x} = d_x - o_x$
          \item Compute $\beta$ for all other nodes using:
            \begin{math}
              \beta_j = \sum_{k}{w_{jk}o_k(1-o_k)\beta_k}
            \end{math}
          \item Compute weight changes for all weight using:
            \begin{math}
              \Delta{w_{ij}} = ro_io_k(1-o_k)\beta_j
            \end{math}
        \end{itemize}
      \item Add up the weight changes to all sample inputs and
        changes the weights
    \end{itemize}
\end{itemize}

\subsection{Back-propagation characteristics}

\begin{itemize}
  \item Training may require thousands of back propagations
  \item Back-propagation can be done in stages
  \item Back-propagation can train a net to learn to recognize
    multiple concepts simultaneously
  \item Trained neural nets can make predictions
  \item Excess weights lead to overfitting. Rule of thumb: be sure
    that the number of trainable weights influencing any
    particular output is smaller than the number of training
    samples
  \item Neural-bet training is an article
\end{itemize}

\subsection{Perceptrons}

\paragraph{Perceptron} A perceptron is a representation, that is a
neural net in which:
\begin{itemize}
  \item There is only one neuron
  \item The input sare binary
  \item Logic boxes may be interposed between the perceptron's
    inputs and the perceptron's weights. Each logic box can be
    viewed as a table that produces an output value of 0s or 1 for
    each combination os 0s and 1s that can appear at its inputs
  \item The output of the perceptron is 0s or 1 depending on
    whether the weighted sum of the logic-box outputs is greater
    than the threshold.
\end{itemize}

The perceptron convergence procedure guarantees success whenever
success is possible.

To train a perceptron:
\begin{itemize}
  \item Until the perceptron yields the correct result for each
    training sample, for each sample,
    \begin{itemize}
      \item If the perceptron yields the wrong answer,
        \begin{itemize}
          \item If the perceptron says no when it should say yes,
            add the logic-box output vector to the weight vector
          \item Otherwise, subtract the logic-box output vector
            from the weight vector
        \end{itemize}
      \item Otherwise, do nothing
    \end{itemize}
\end{itemize}

\section{Genetic Algorithms}

\paragraph{Chromosome} A chromosome is a representation in which:
\begin{itemize}
  \item There is a list of lements called genes
  \item The chromosome determines the overall fitness manifested by
    some mechanism that uses the chromosome's genes as a sort of
    blueprint
\end{itemize}
With constructors that:
\begin{itemize}
  \item Create a chromosome, given a list of elements; this
    constructor might be called the \textit{genesis constructor}
  \item Create a chromosome by crossing a pair of existing
    chromosomes
\end{itemize}
With writers that:
\begin{itemize}
  \item Mutate an existing chromosome by changing one of the genes
\end{itemize}
With readers that:
\begin{itemize}
  \item Produce a specified gene, given a chromosome
\end{itemize}

\subsection{Fitness}

The \textbf{standard method} for fitness computation is:
\begin{math}
  f_i = \frac{q_i}{\sum_{j}{q_j}}
\end{math}

The \textbf{rank method} for fitness computation is:
\begin{itemize}
  \item Sort the $n$ individuals by quality
  \item Let the probability of selecting the $i$th candidate, given
    that the first $i-1$ candidates have not been selected, be $p$,
    except for the final candidate, which is selected if no
    previous candidate has been selected
  \item Select a candidate using the computed probabilities
\end{itemize}

The \textbf{rank-space method} for fitness computation is:
\begin{itemize}
  \item Sort the $n$ individuals by quality
  \item Sort the $n$ individuals by the sum of their inverse
    squared distances to already selected candidates (the lower the
    sum, the better the rank)
  \item Use the rank method, but sort on the sum of the quality
    rank and the diversity rank, rather than on quality rank only
\end{itemize}

\section{Crossover}

\begin{itemize}
  \item Crossover enables to search high-dimensional spaces efficiently; 
it reduces the dimensionality of the optimum search space
  \item Crossover enables genetic algorithms to travers obstructing
    moats
\end{itemize}

\subsection{Natural selection}

To mimic natural selection in general:
\begin{itemize}
  \item Create an initial population of one chromosome
  \item Mutate one or more genes in one or more of the current
    chromosomes, producitng one new offspring for each chromosome
    mutated
  \item Mate one or more pairs of chromosomes
  \item Add the mutated na doffspring chromosomes to the current
    population
  \item Create a new generation by keeping the best of the current
    population's chromosomes, along with other chromosomes selected
    randomly from the current population. Bias the random selection
    according to assessed fitness
\end{itemize}

\section{Near Misses and Felicity Conditions}


\paragraph{Near miss} A \textit{near miss} is a negative example
that, for a small number of reasons, is not an instance of the
class being taught.

\subsection{Heuristics list}
\begin{itemize}
  \item The \textbf{require-link} heuristic is used when an
    evolving model has a link in a place where a near miss does
    not. The model link is converted to a \textit{Must} form
  \item The \textbf{forbid-link} heuristic is used when a near miss
    has a link in a place where and evolving model does not. A
    \textit{Must-not} form is installed in te evolving model
  \item The \textbf{climb-tree} heuristic is used when an object in
    an evolving model corresponds to a different object in an
    example. \textit{Must-be-a} links are routed to the most
    specific common class in the classification tree above the
    model object and the example object
  \item The \textbf{enlarge-set} heuristic is used when an object
    in an evolving model corresponds to a different object in an
    example and the two objexts are not related to each other
    through a classification tree. \textit{Must-be-a} links are
    routed to a new class composed of the union of the objects'
    classes
  \item The \textbf{drop-link} heuristic is used when the objects
    that are different in an evolving model and in ana example form
    an exhaustive set. The drop-link heuristic is also used when
    an evolving model has a link that is not in the example. The
    link is dropped from the model
  \item The \textbf{close-interval} heuristic is used when a number
    or interval in an evolving model corresponds to a number in an
    example. If the model uses a number, the number is replaced by
    an interval spanning the model's number and the example's
    number. If the model uses an interval, the interval is enlarged
    to read the example's number
\end{itemize}

\subsection{Procedures}

\paragraph{SPECIALIZE}
\begin{itemize}
  \item Match the evolving model to the example to establish
    correspondences among parts
  \item Determine whether there is a single, most important
    difference between the evolving model and the near miss
  \begin{itemize}
    \item If there is a single, most important difference,
      \begin{itemize}
        \item If the evolving model has a link that is not in the
          near miss, use the require-link heuristic
        \item If the near miss has a link that is not in the model,
          use the forbid-link heuristic
      \end{itemize}
    \item Otherwise, ignore the example
  \end{itemize}
\end{itemize}

\paragraph{GENERALIZE}
\begin{itemize}
  \item Match the evolving model to the example to establish
    correspondences among parts
  \item For each difference, determine the difference type:
    \begin{itemize}
      \item If a link points to a class in the evolving model
        different from the class to which the link points in the
        example:
        \begin{itemize}
          \item If the classes are part of a class tree, use the
            climb-tree heuristic
          \item If the classes for an exhaustive set, use the
            drop-link heuristic
          \item Otherwise, use the enlarge-set heuristic
        \end{itemize}
      \item If a link is missing in the exmaple, use the drop-link
        heuristic
      \item If the difference is that different numbers, or an
        interval and a number out the interval, are involved, use
        the close-interval heuristic
      \item Otherwise, ignore the difference
    \end{itemize}
\end{itemize}

Notes:
\begin{itemize}
  \item Specialize does nothing if it cannot identify a most
    important difference. You need a procedure that ranks all
    differes by difference type and by link type
  \item Both SPECIALIZE and GENERALIZE involve matching. You need a
    matching procedure
\end{itemize}

\subsection{Learning}

To learn using procedure W:
\begin{itemize}
  \item Let the description of the first example, which must be an
    example, be the initial description
  \item For all subsection examples:
    \begin{itemize}
      \item If the example is a near miss, use SPECIALIZE
      \item If the example is an example, use GENERALIZE
    \end{itemize}
\end{itemize}

\paragraph{Wait-and-see principle} If there is doubt about 
what to do, do nothing.

Learning-facilitating teacher-student agreements are called
\textbf{felicity conditions}.

\paragraph{No-altering principle} When an object ir situation known
to be an example fails to match a general model, create a
special-case exception model.

\subsection{Identification}

\paragraph{Similarity net} A similarity net is a representation
that is a semantic net in which:\begin{itemize}
  \item Nodes denotes models
  \item Links connect similar models
  \item Links are tied to difference descriptions
\end{itemize}


























\end{document}
